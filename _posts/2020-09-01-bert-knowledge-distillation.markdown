---
layout: post
title: "Knowledge Distillation in BERT"
authors: "<strong>Daejin Kim</strong>"
date: 2019-08-01
image: /images/bert-kd.png
categories: project
# venue: "CIKM 2022"
# paper: https://arxiv.org/abs/2209.05406
---
· Implement a smaller version of BERT (with 4 layers and 8 attention heads) by using knowledge distillation.<br>
· Significantly reduce the number of parameters of BERT while minimizing performance degradation.